/*Copyright 2021 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
syntax = "proto2";

package mint.protos;

// Message for configuring multi_modal training jobs.
message TrainConfig {
  optional int32 num_steps = 1 [default = 10000];
  optional int32 batch_size = 2 [default = 4];

  // Whether to use bfloat16 for training (on TPU).
  optional bool use_bfloat16 = 3 [default = false];

  // Learning rate.
  optional LearningRate learning_rate = 4;
  optional float grad_clip_norm = 5 [default = 1.0];

  // Checkpoint to restore variables from.
  optional string fine_tune_checkpoint = 6 [default = ""];

  // Options for checkpoint type. DEFAULT will load all the variables.
  // MOTION_MODEL will only load the variables for motion model.
  enum CheckpointType {
    DEFAULT = 0;
  }
  optional CheckpointType fine_tune_checkpoint_type = 7;
}

// Configuration message for optimizer learning rate.
message LearningRate {
  oneof learning_rate {
    ConstantLearningRate constant_learning_rate = 1;
    ExponentialDecayLearningRate exponential_decay_learning_rate = 2;
    ManualStepLearningRate manual_step_learning_rate = 3;
    CosineDecayLearningRate cosine_decay_learning_rate = 4;
  }
}

// Configuration message for a constant learning rate.
message ConstantLearningRate {
  optional float learning_rate = 1 [default = 0.002];
}

// Configuration message for an exponentially decaying learning rate.
// See https://www.tensorflow.org/versions/master/api_docs/python/train/ \
//     decaying_the_learning_rate#exponential_decay
message ExponentialDecayLearningRate {
  optional float initial_learning_rate = 1 [default = 0.002];
  optional uint32 decay_steps = 2 [default = 4000000];
  optional float decay_factor = 3 [default = 0.95];
  optional bool staircase = 4 [default = true];
  optional float burnin_learning_rate = 5 [default = 0.0];
  optional uint32 burnin_steps = 6 [default = 0];
  optional float min_learning_rate = 7 [default = 0.0];
}

// Configuration message for a manually defined learning rate schedule.
message ManualStepLearningRate {
  optional float initial_learning_rate = 1 [default = 0.002];
  message LearningRateSchedule {
    optional uint32 step = 1;
    optional float learning_rate = 2 [default = 0.002];
  }
  repeated LearningRateSchedule schedule = 2;

  // Whether to linearly interpolate learning rates for steps in
  // [0, schedule[0].step].
  optional bool warmup = 3 [default = false];
}

// Configuration message for a cosine decaying learning rate as defined in
// https://github.com/tensorflow/models/blob/master/research/object_detection/utils/learning_schedules.py
message CosineDecayLearningRate {
  optional float learning_rate_base = 1 [default = 0.002];
  optional uint32 total_steps = 2 [default = 4000000];
  optional float warmup_learning_rate = 3 [default = 0.0002];
  optional uint32 warmup_steps = 4 [default = 10000];
  optional uint32 hold_base_rate_steps = 5 [default = 0];
}
