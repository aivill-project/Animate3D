/*Copyright 2021 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/
syntax = "proto2";

package mint.protos;

import "mint/protos/preprocessor.proto";

message MultiModalModel {
  oneof model {
    FACTModel fact_model = 1;
  }
}

message FACTModel {
  repeated Modality modality = 1;
  optional CrossModalModel cross_modal_model = 2;
  optional string fk_path = 3;
}

message ModalityInputConfig {
  optional bool use_look_ahead_mask = 2 [default = false];
}

message Modality {
  optional string feature_name = 1;
  optional int32 feature_dim = 2;
  optional int32 sequence_length = 3;
  optional ModalityInputConfig input_config = 4;
  repeated ModalityPreprocessor preprocessor = 5;
  repeated ModalityModel model = 6;
}

message ModalityModel {
  oneof model {
    Transformer transformer = 1;
    MLP mlp = 2;
  }
}

message MLP {
  optional float initializer_range = 1 [default = 0.02];
  optional string hidden_act = 2 [default = "gelu"];
  // Output dimension.
  optional int32 out_dim = 3;
}

message Conv2D {
  optional int32 hidden_size = 1 [default = 512];
  optional int32 kernel_size = 2 [default = 16];
}

message ModalityPreprocessor {
  oneof preprocessor {
    FACTPreprocessor fact_preprocessor = 1;
  }
}

message CrossModalModel {
  // The cross modal function.
  optional string modality_a = 1;
  optional string modality_b = 2;
  oneof model {
    Transformer transformer = 3;
    MLP mlp = 4;
  }

  // Cross modal concatenate dimension
  enum CrossModalConcatDim {
    DEFAULT_CONCAT = 0;
    SEQUENCE_WISE = 1;
    CHANNEL_WISE = 2;
  }
  optional CrossModalConcatDim cross_modal_concat_dim = 5
      [default = SEQUENCE_WISE];
  // Output MLP layer config.
  optional MLP output_layer = 6;

  // Preprocess each feature if necessary.
  enum Preprocess {
    DEFAULT_NONE = 0;
    // Prepare feature for mutual information loss.
    CONTRASTIVE = 1;
  }
  optional Preprocess preprocess = 7;
}

message Transformer {
  // Modality specific parameters.
  optional int32 hidden_size = 1 [default = 768];
  optional int32 num_hidden_layers = 2 [default = 12];
  optional int32 num_attention_heads = 3 [default = 12];
  optional int32 max_position_embeddings = 4 [default = 512];

  // Common params for transformer.
  optional int32 intermediate_size = 5 [default = 3072];
  optional string hidden_act = 6 [default = "gelu"];
  optional float hidden_dropout_prob = 7 [default = 0.1];
  optional float attention_probs_dropout_prob = 8 [default = 0.1];
  optional float initializer_range = 9 [default = 0.02];
  optional string masked_loss_type = 10 [default = "nce"];

  // Parameters for spatial transformer.
  optional bool add_spatial_attention = 11 [default = false];
  optional int32 sp_hidden_size = 12 [default = 768];
  optional int32 sp_num_attention_heads = 13 [default = 12];
  optional int32 sp_num_hidden_layers = 14 [default = 12];

  // Whether or not concat the input with a cls token.
  optional bool add_cls_token = 15 [default = false];
  optional float weight_decay = 16 [default = 0.];
}
